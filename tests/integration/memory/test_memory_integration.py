"""Integration tests for the NeuroCognitive Architecture memory systems.

These tests verify the interactions between Working Memory, Episodic Memory, 
and Semantic Memory, focusing on:
1. Information flow across memory tiers
2. End-to-end consolidation processes
3. Pattern recognition and abstraction
4. Biological constraints in the integrated system
"""

import time

import pytest

# Import the concrete implementation instead of the abstract base class
from neuroca.core.memory.consolidation import StandardMemoryConsolidator
from neuroca.core.memory.episodic_memory import EpisodicMemory
from neuroca.core.memory.factory import create_memory_system
from neuroca.core.memory.working_memory import WorkingMemory
from neuroca.memory.semantic_memory import SemanticMemory  # Corrected import path

# Assuming Concept, Relationship, RelationshipType might be defined elsewhere or not needed directly if SemanticMemory handles them internally
# If they are needed and defined in core, keep that part of the import or find their correct location.
# For now, focusing on fixing the SemanticMemory instantiation.
# from neuroca.core.memory.semantic_memory import Concept, Relationship, RelationshipType # Example if needed from core


@pytest.fixture()
def memory_system():
    """Provides a complete memory system with all three tiers and consolidation.
    
    Returns:
        Tuple containing (working_memory, episodic_memory, semantic_memory, consolidator)
    """
    working_memory = WorkingMemory()
    episodic_memory = EpisodicMemory()
    semantic_memory = SemanticMemory()
    # Instantiate the concrete class
    consolidator = StandardMemoryConsolidator() 
    
    yield (working_memory, episodic_memory, semantic_memory, consolidator)
    
    # Clean up
    working_memory.clear()
    episodic_memory.clear()
    semantic_memory.clear()


def test_working_to_episodic_consolidation(memory_system):
    """Test that high-activation memories consolidate from working to episodic memory."""
    working_memory, episodic_memory, _, consolidator = memory_system
    
    # Store high-activation memories in working memory
    memory_content = "Important meeting tomorrow at 10am"
    chunk_id = working_memory.store(memory_content, activation=0.9, 
                                    metadata={"priority": "high"})
    
    # Verify memory is in working memory
    chunk = working_memory.retrieve_by_id(chunk_id)
    assert chunk is not None
    assert chunk.content == memory_content
    
    # Consolidate high-activation memories to episodic memory
    # Use the correct consolidate method
    consolidated_ids = consolidator.consolidate(
        source=working_memory, target=episodic_memory
    )
    consolidated = len(consolidated_ids) # Get the count
    
    # Verify memory was consolidated to episodic memory
    assert consolidated == 1
    
    # Check that memory exists in episodic memory with proper metadata
    episodes = episodic_memory.get_all_items() # Use correct method
    assert len(episodes) == 1
    assert memory_content in [chunk.content for chunk in episodes]
    
    # Verify that metadata was preserved
    episode = [chunk for chunk in episodes if chunk.content == memory_content][0]
    assert "priority" in episode.metadata
    assert episode.metadata["priority"] == "high"
    
    # Verify temporal context was added
    assert "timestamp" in episode.metadata


def test_emotional_content_preservation(memory_system):
    """Test that emotional salience is preserved during consolidation."""
    working_memory, episodic_memory, _, consolidator = memory_system
    
    # Store memory with emotional content
    emotional_memory = "Received great news about the promotion"
    working_memory.store(emotional_memory, activation=0.8, 
                                    metadata={"emotional_salience": 0.9})
    
    # Consolidate to episodic memory
    # Use the correct consolidate method
    consolidator.consolidate(source=working_memory, target=episodic_memory)
    
    # Verify emotional salience was preserved
    episodes = episodic_memory.get_all_items() # Use correct method
    episode = [chunk for chunk in episodes if chunk.content == emotional_memory][0]
    
    assert "emotional_salience" in episode.metadata
    assert episode.metadata["emotional_salience"] == 0.9


def test_repeated_experiences_form_concepts(memory_system):
    """Test that repeated similar experiences in episodic memory form concepts in semantic memory."""
    _, episodic_memory, semantic_memory, consolidator = memory_system
    
    # Create sequence of related memories about dogs
    dog_memories = [
        "Saw a golden retriever at the park",
        "My neighbor's dog barked all night",
        "Watched a documentary about different dog breeds",
        "Read an article about training dogs",
        "Played with a friend's puppy yesterday"
    ]
    
    # Store these in episodic memory with timestamps spaced apart
    base_time = time.time() - 86400 * 30  # 30 days ago
    for i, memory in enumerate(dog_memories):
        # Space memories out by 3 days each
        timestamp = base_time + (i * 86400 * 3)
        episodic_memory.store(memory, metadata={
            "timestamp": timestamp,
            "emotional_salience": 0.5,
            "tags": ["dog", "animals", "experience"]
        })
    
    # Consolidate patterns from episodic to semantic memory
    # Use the correct consolidate method
    consolidator.consolidate(
        source=episodic_memory, target=semantic_memory
    )
    
    # Verify that a dog concept was formed in semantic memory
    concepts = semantic_memory.retrieve_all_concepts()
    
    # Should have at least created a dog concept
    assert any(concept for concept in concepts if "dog" in concept.name.lower())
    
    # The concept should have properties derived from experiences
    dog_concept = next(concept for concept in concepts if "dog" in concept.name.lower())
    assert len(dog_concept.properties) > 0
    assert dog_concept.properties.get("animal", False) or dog_concept.properties.get("pet", False)


def test_end_to_end_information_flow(memory_system):
    """Test end-to-end flow of information through all memory tiers."""
    working_memory, episodic_memory, semantic_memory, consolidator = memory_system
    
    # 1. Store several memories about fruits in working memory
    fruit_memories = [
        ("Apples are sweet and crunchy", 0.8, {"category": "food", "subcategory": "fruit"}),
        ("Oranges contain vitamin C", 0.7, {"category": "food", "subcategory": "fruit"}),
        ("Bananas are high in potassium", 0.75, {"category": "food", "subcategory": "fruit"})
    ]
    
    for content, activation, metadata in fruit_memories:
            working_memory.store(content, activation=activation, metadata=metadata)
    
    # 2. Consolidate from working to episodic memory
    # Use the correct consolidate method
    w_to_e_ids = consolidator.consolidate(
        source=working_memory, target=episodic_memory
    )
    w_to_e_count = len(w_to_e_ids)
    assert w_to_e_count >= 3
    
    # 3. Add more fruit experiences directly to episodic memory over "time"
    more_fruits = [
        ("Strawberries are my favorite summer fruit", {"timestamp": time.time() - 86400 * 10}),
        ("Blueberries are considered a superfood", {"timestamp": time.time() - 86400 * 8}),
        ("Mangoes are tropical fruits", {"timestamp": time.time() - 86400 * 5})
    ]
    
    for content, metadata in more_fruits:
        metadata["category"] = "food"
        metadata["subcategory"] = "fruit"
        episodic_memory.store(content, metadata=metadata)
    
    # 4. Consolidate from episodic to semantic memory
    # Use the correct consolidate method
    e_to_s_ids = consolidator.consolidate(
        source=episodic_memory, target=semantic_memory
    )
    len(e_to_s_ids)
    
    # 5. Verify that semantic memory contains fruit concept
    concepts = semantic_memory.retrieve_all_concepts()
    fruit_concept = next((c for c in concepts if c.name.lower() == "fruit"), None)
    
    assert fruit_concept is not None
    
    # 6. Verify properties of the fruit concept
    assert "edible" in fruit_concept.properties or "food" in fruit_concept.properties
    
    # 7. Verify relationships to specific fruits
    relationships = semantic_memory.retrieve_relationships_for_concept(fruit_concept.id)
    
    # Should have relationships to specific fruits or categories
    assert len(relationships) > 0


def test_working_memory_capacity_maintained(memory_system):
    """Test that working memory capacity constraints are maintained during integration."""
    working_memory, _, _, _ = memory_system
    
    # Get initial capacity (should be around 7Â±2 items)
    capacity = working_memory.capacity
    
    # Try to store twice the capacity
    for i in range(capacity * 2):
        working_memory.store(f"Memory item {i}", activation=0.5)
    
    # Verify that capacity wasn't exceeded
    chunks = working_memory.get_all_items() # Use correct method
    assert len(chunks) <= capacity
    
    # Cannot test consolidation here as we only have the working memory
    # instance, not the full system with episodic memory and consolidator
    
    # Just verify capacity is maintained
    chunks = working_memory.get_all_items() # Use correct method
    assert len(chunks) <= capacity


def test_episodic_temporal_sequence_preserved(memory_system):
    """Test that temporal sequence of episodes can be reconstructed."""
    working_memory, episodic_memory, _, consolidator = memory_system
    
    # Create a sequence of memories about a vacation
    vacation_memories = [
        ("Packed for the beach vacation", time.time() - 86400 * 7),
        ("Drove to the airport", time.time() - 86400 * 6),
        ("Took a flight to Hawaii", time.time() - 86400 * 6 + 3600 * 3),
        ("Checked into the beach resort", time.time() - 86400 * 6 + 3600 * 8),
        ("Went swimming in the ocean", time.time() - 86400 * 5),
        ("Had a beachside dinner", time.time() - 86400 * 5 + 3600 * 6),
        ("Went hiking to a waterfall", time.time() - 86400 * 4),
        ("Took a boat tour around the island", time.time() - 86400 * 3),
        ("Packed for the trip home", time.time() - 86400 * 2),
        ("Flew back home", time.time() - 86400)
    ]
    
    # Store these with sequence metadata
    sequence_id = "hawaii-vacation-2025"
    for i, (content, timestamp) in enumerate(vacation_memories):
        episodic_memory.store(content, metadata={
            "timestamp": timestamp,
            "sequence_id": sequence_id,
            "sequence_index": i,
            "location": "Hawaii" if i >= 2 and i <= 8 else "Home"
        })
    
    # Consolidate to potentially form a "vacation" concept
    # Use the correct consolidate method
    consolidator.consolidate(source=episodic_memory, target=semantic_memory)
    
    # Retrieve the vacation sequence in order
    vacation_sequence = episodic_memory.retrieve_by_metadata(
        {"sequence_id": sequence_id}, 
        sort_by="sequence_index"
    )
    
    # Verify sequence is complete and in order
    assert len(vacation_sequence) == len(vacation_memories)
    
    # Check that ordering is preserved
    for i in range(len(vacation_sequence) - 1):
        assert vacation_sequence[i].metadata["sequence_index"] < vacation_sequence[i+1].metadata["sequence_index"]
    
    # Retrieve only Hawaii memories
    hawaii_memories = episodic_memory.retrieve_by_metadata({"location": "Hawaii"})
    assert len(hawaii_memories) == 7  # Should be 7 memories in Hawaii


def test_semantic_inference(memory_system, populated_knowledge_graph):
    """Test that semantic memory can infer new relationships based on existing knowledge."""
    working_memory, episodic_memory, semantic_memory, _ = memory_system
    
    # Replace semantic memory with pre-populated knowledge graph
    semantic_memory.clear()
    
    # Copy all concepts and relationships from populated graph
    for concept in populated_knowledge_graph.retrieve_all_concepts():
        semantic_memory.store(concept)
        
    for relationship in populated_knowledge_graph.retrieve_all_relationships():
        semantic_memory.store(relationship)
    
    # Query inferred properties - a golden retriever should inherit dog properties
    golden_properties = semantic_memory.get_concept_properties("golden", include_inherited=True)
    
    # Should have both its own properties and those inherited from "dog"
    assert "friendly" in golden_properties  # Direct property
    assert "domesticated" in golden_properties  # Inherited from dog
    
    # Test multiple inheritance levels - Berlin should be in a country which is a location
    berlin_concept = semantic_memory.get_concept("berlin")
    assert berlin_concept is not None
    
    # Find all Berlin's relationships
    berlin_relationships = semantic_memory.retrieve_relationships_for_concept("berlin")
    
    # Should have IS_A city and LOCATED_IN Germany relationships
    assert any(r.relationship_type == RelationshipType.IS_A and r.target_id == "city" 
               for r in berlin_relationships)
    assert any(r.relationship_type == RelationshipType.LOCATED_IN and r.target_id == "germany" 
               for r in berlin_relationships)
    
    # Verify that berlin inherits the "location" property through multiple levels
    all_properties = semantic_memory.get_concept_properties("berlin", include_inherited=True)
    assert len(all_properties) > 0


def test_stress_memory_system(memory_system):
    """Test memory systems under load with many operations."""
    working_memory, episodic_memory, semantic_memory, consolidator = memory_system
    
    # Generate a large number of memories
    num_memories = 100
    
    # 1. Rapidly store many items in working memory (will exceed capacity)
    for i in range(num_memories):
        activation = 0.5 + (i % 5) * 0.1  # Vary activation levels
        working_memory.store(f"Working memory item {i}", 
                            activation=activation,
                            metadata={"category": f"category-{i % 10}"})
    
    # Working memory should not exceed capacity
    assert len(working_memory.get_all_items()) <= working_memory.capacity # Use correct method
    
    # 2. Consolidate to episodic memory several times
    total_consolidated = 0
    for _ in range(3):
        consolidated = consolidator.consolidate_working_to_episodic(
            working_memory, episodic_memory
        )
        total_consolidated += consolidated
        
        # Add more items to working memory
        for i in range(10):
            working_memory.store(f"New working memory item {i}", 
                                activation=0.8,
                                metadata={"urgent": True})
    
    # 3. Store many episodic memories directly
    for i in range(num_memories):
        timestamp = time.time() - (num_memories - i) * 3600  # Spaced over time
        episodic_memory.store(f"Episodic memory {i}",
                             metadata={
                                 "timestamp": timestamp,
                                 "emotional_salience": 0.4 + (i % 6) * 0.1,
                                 "category": f"category-{i % 5}"
                             })
    
    # 4. Consolidate to semantic memory
    consolidator.consolidate_episodic_to_semantic(
        episodic_memory, semantic_memory
    )
    
    # Should have formed some concepts
    assert len(semantic_memory.retrieve_all_concepts()) > 0
    
    # 5. Verify working memory still maintains constraints
    assert len(working_memory.get_all_items()) <= working_memory.capacity # Use correct method


def test_adaptive_consolidation(memory_system):
    """Test that consolidation adapts based on emotional content and other factors."""
    working_memory, episodic_memory, semantic_memory, consolidator = memory_system
    
    # Create memories with different emotional salience
    emotional_memories = [
        ("Just a regular thought", 0.5, {"emotional_salience": 0.1}),
        ("Something interesting happened", 0.6, {"emotional_salience": 0.4}),
        ("That was really exciting!", 0.7, {"emotional_salience": 0.7}),
        ("That was the most terrifying moment of my life", 0.8, {"emotional_salience": 0.9})
    ]
    
    # Store in working memory
    for content, activation, metadata in emotional_memories:
        working_memory.store(content, activation=activation, metadata=metadata)
    
    # Set consolidator to only consolidate highly emotional items
    consolidator.working_to_episodic_threshold = 0.65
    # Note: These parameters might need to be passed to the consolidate method if they aren't instance attributes
    # consolidator.working_to_episodic_threshold = 0.65 # Assuming these are set via constructor or setters
    # consolidator.emotional_salience_boost = 0.2  # Assuming this affects internal logic
    consolidator.set_activation_threshold(0.65) # Use setter if available
    # emotional_salience_boost seems not directly settable, it might be implicit logic
    
    # Run consolidation
    # Use the correct consolidate method
    consolidated_ids = consolidator.consolidate(
        source=working_memory, target=episodic_memory
        # Pass thresholds if needed: activation_threshold=0.65, emotional_threshold=...
    )
    len(consolidated_ids)
    
    # Should have consolidated only the high emotion/activation items
    episodes = episodic_memory.get_all_items() # Use correct method
    episode_contents = [e.content for e in episodes]
    
    # The two highest emotional items should be consolidated
    assert "That was really exciting!" in episode_contents
    assert "That was the most terrifying moment of my life" in episode_contents
    
    # The lowest emotional item should not be consolidated
    assert "Just a regular thought" not in episode_contents


def test_health_based_memory_performance(realistic_memory_load):
    """Test memory performance under different health conditions."""
    working_memory, episodic_memory, semantic_memory, consolidator = realistic_memory_load
    
    # Simulate reduced cognitive capacity due to health factors
    original_capacity = working_memory.capacity
    working_memory.capacity = max(3, int(original_capacity * 0.6))  # Reduce to 60%
    
    # Try to store more items than the reduced capacity
    for i in range(working_memory.capacity + 3):
        working_memory.store(f"Health-impacted memory {i}", activation=0.7)
    
    # Should not exceed the reduced capacity
    assert len(working_memory.get_all_items()) <= working_memory.capacity # Use correct method
    
    # Retrieval should prioritize highest activation under constrained conditions
    high_importance = "CRITICAL: Remember this information"
    working_memory.store(high_importance, activation=0.95)
    
    # The high importance item should be present despite capacity constraints
    assert high_importance in [chunk.content for chunk in working_memory.get_all_items()] # Use correct method
    
    # Return to normal capacity
    working_memory.capacity = original_capacity


# --- Tests for Memory Factory ---

def test_create_working_memory_via_factory():
    """Test creating WorkingMemory using the factory."""
    # WorkingMemory capacity must be between 5-9 chunks
    memory = create_memory_system("working", capacity=7)
    assert isinstance(memory, WorkingMemory)
    assert memory.capacity == 7

def test_create_episodic_memory_via_factory():
    """Test creating EpisodicMemory using the factory."""
    # EpisodicMemory doesn't accept capacity parameter
    memory = create_memory_system("episodic")
    assert isinstance(memory, EpisodicMemory)

def test_create_working_memory_capacity_validation():
    """Test that WorkingMemory enforces capacity constraints."""
    # Should reject capacity outside the 5-9 range
    with pytest.raises(ValueError, match="Working memory capacity must be between 5-9"):
        create_memory_system("working", capacity=10)
    
    # But should accept capacity within the range
    memory = create_memory_system("working", capacity=5)
    assert memory.capacity == 5

def test_create_unknown_memory_type_raises_error():
    """Test that requesting an unknown memory type raises an error."""
    with pytest.raises(ValueError):  # Factory raises ValueError for unknown types
        create_memory_system("unknown_type")

# Note: The aliases and semantic memory tests need implementation inspection
# to determine why they're failing. They might not be properly implemented yet.
